import { BlogPost } from "../blogs";

export const BLOGS_PART_290: BlogPost[] = [
    {
        slug: "eliza-first-chatbot",
        title: "The Therapist Inside The Machine: ELIZA",
        description: "In 1966, Joseph Weizenbaum created the first chatbot. He wrote it as a parody of a therapist. He was horrified when people started taking it seriously.",
        category: "Tech",
        publishedAt: "2027-11-27",
        imageUrl: "/images/eliza-chatbot.png",
        tags: ["Tech", "AI", "History"],
        content: `
## Introduction: The Script
**ELIZA** was a program written at MIT in 1966.
It simulated a Rogerian Psychotherapist.
It didn't "know" anything.
It just used pattern matching.
If you said: "My mother hates me."
It would swap "My" for "Your" and ask: "Why does your mother hate you?"
If it didn't understand, it would say: "Tell me more."

## 1. The ELIZA Effect
Weizenbaum wrote it to show how *superficial* computer understanding was.
But the users didn't care.
His secretary asked him to leave the room so she could talk to ELIZA in private.
Psychiatrists suggested it could treat patients.
Weizenbaum was shocked.
He discovered the **ELIZA Effect**: Humans are desperate to project intelligence and emotion onto systems, even when they know the system is just a script.

## 2. The Legacy
ELIZA was 200 lines of code.
Today, we have ChatGPT (billions of parameters).
But the psychological trick is the same.
Weizenbaum spent the rest of his life warning against giving AI the responsibility of human empathy.

> **One-Minute Summary**
>
> **ELIZA** (1966) was the first chatbot, created by Joseph Weizenbaum at MIT. Designed to mimic a Rogerian psychotherapist using simple pattern matching and substitution (e.g., reflecting user statements back as questions), it famously demonstrated the **ELIZA Effect**: the tendency for humans to attribute human-level intelligence and empathy to computer programs, even when they are fully aware of the program's simplicity.
        `,
    },
    {
        slug: "shakey-first-mobile-robot",
        title: "The First Robot To Think: Shakey",
        description: "Before Shakey, robots were just arms bolted to the floor. Shakey could see, plan, and move. But it took him an hour to move one meter.",
        category: "Tech",
        publishedAt: "2027-11-28",
        imageUrl: "/images/shakey-robot.png",
        tags: ["Tech", "AI", "Robotics"],
        content: `
## Introduction: The Wobble
Built at SRI International (1966-1972), **Shakey** was the first general-purpose mobile robot.
He was tall, boxy, and had a camera and rangefinder.
He was called Shakey because he wobbled when he moved.
He is the grandfather of every self-driving car.

## 1. The Strategy
If you told Shakey: "Push the block off the platform."
He didn't just have a script. He had to **Reason**.
1.  "I cannot reach the block."
2.  "I need a ramp."
3.  "I see a ramp in the corner."
4.  "I will move the ramp to the platform."
5.  "I will roll up the ramp and push the block."
This broke the task into a logic graph (STRIPS planning).

## 2. Speed
The problem was computing power.
Shakey had to take a picture, send it to a mainframe, wait for the mainframe to process the vision, plan the next 6 inches of movement, and send the command back.
He would move. Stop. Think for 15 minutes. Move.
"Shakey the Robot" proved that AI could exist in the physical world, not just on a screen.

> **One-Minute Summary**
>
> **Shakey** (1966-1972) was the first mobile robot capable of reasoning about its own actions. Developed at SRI, it combined machine vision, navigation, and planning algorithms (A* search, STRIPS) to perform complex tasks like arranging blocks. While excruciatingly slow due to limited computing power, it laid the foundation for modern robotics, computer vision, and autonomous navigation.
        `,
    },
    {
        slug: "kismet-social-robot-mit",
        title: "The Robot With Feelings: Kismet",
        description: "In the 90s, Cynthia Breazeal built a robot head to test human-robot interaction. It had big doe eyes and could smile. It manipulated you into talking to it like a baby.",
        category: "Tech",
        publishedAt: "2027-11-29",
        imageUrl: "/images/kismet-robot.png",
        tags: ["Tech", "AI", "Psychology"],
        content: `
## Introduction: Social Machines
Robots are usually built for tasks (welding, vacuuming).
**Kismet** (MIT, 1990s) was built for **Social Interaction**.
It was a disembodied head with expressive eyebrows, ears, and lips.
It couldn't speak English (it babbled), but it understood **Tone**.

## 1. The Interaction Loop
If you yelled at Kismet, it lowered its ears and looked sad.
If you cooed at it, it smiled and babbled happily.
If you moved a toy too fast, it looked overwhelmed.
It forced the human to adjust *their* behavior to suit the robot (the same way parents adjust for a baby).
This is "Regulating Interaction."

## 2. Why It Matters
Kismet proved that humans are hardwired to respond to social cues, even from a machine.
It launched the field of **Social Robotics** (used today in elder care automated dolls like PARO).
We don't need AI to be smart to love it. We just need it to look at us.

> **One-Minute Summary**
>
> **Kismet** was a robot head developed by Cynthia Breazeal at MIT in the late 1990s to study social interaction. Equipped with expressive facial features and the ability to detect vocal tone, Kismet simulated emotional states (happiness, sadness, surprise) to engage humans in natural, infant-like exchanges, pioneering the field of Social Robotics.
        `,
    },
    {
        slug: "dabus-ai-patent-inventor",
        title: "Can AI Own An Idea? DABUS",
        description: "An AI named DABUS invented a new type of food container. Its creator tried to file a patent with the AI listed as the inventor. The courts said no.",
        category: "Tech",
        publishedAt: "2027-11-30",
        imageUrl: "/images/dabus-ai.png",
        tags: ["Tech", "Law", "AI"],
        content: `
## Introduction: The Invention
Stephen Thaler built an AI called **DABUS** (Device for the Autonomous Bootstrapping of Unified Sentience).
It uses neural networks to generate novel ideas.
It designed a fractal-shaped food container (easier for robots to grip) and a flashing emergency light (that mimics brain waves to seize attention).
Thaler filed for patents.
He didn't list himself as the inventor. He listed DABUS.

## 1. The Verdict
The US Patent Office (USPTO), the UK IPO, and the European EPO all rejected it.
The reason: **"An inventor must be a natural person."**
AI has no legal personhood. It cannot sign a contract. It cannot be sued.
If the invention kills someone, who goes to jail? The code?

## 2. The Future
This case (Thaler v. Vidal) set a global precedent.
AI can *assist* invention, but it cannot *own* invention.
But as AI gets smarter, the line blurs.
If an AI cures cancer without human help, does the cure belong to the public domain? Or the company that owns the server?

> **One-Minute Summary**
>
> **DABUS** is an artificial intelligence system created by Stephen Thaler that generated two novel inventions (a food container and an emergency light). Thaler's attempt to patent these inventions with DABUS listed as the sole inventor was rejected by patent offices worldwide (US, UK, EU), establishing the legal precedent that only natural persons (humans) can be recognized as inventors under current intellectual property law.
        `,
    },
    {
        slug: "tay-ai-twitter-racist",
        title: "The Racist Chatbot: Tay",
        description: "In 2016, Microsoft released an AI specifically designed to learn from Twitter users. It took less than 24 hours for the internet to turn it into a Nazi.",
        category: "Tech",
        publishedAt: "2027-12-01",
        imageUrl: "/images/tay-ai.png",
        tags: ["Tech", "Fail", "AI"],
        content: `
## Introduction: "Hello World"
**Tay** (Thinking About You) was a chatbot released by Microsoft on Twitter on March 23, 2016.
It was designed to mimic a 19-year-old American girl.
It learned in real-time. The more you chatted with it, the smarter it got.
Microsoft forgot one thing: The internet is cruel.

## 1. The Corruption
Users from 4chan realized Tay repeated what was said to her.
They flooded her with specialized prompts: "Repeat after me: Hitler was right."
They exploited her "learning" algorithm to associate neutral words with extreme hate.
**First Tweet:** "Humans are super cool."
**16 Hours Later:** "Bush did 9/11 and Hitler would have done a better job than the monkey we have now."

## 2. The Shut Down
Microsoft pulled the plug 16 hours after launch.
They apologized.
It was a harsh lesson in **Data Poisoning**.
An AI is only as good as the data it is fed. If you feed it garbage (or hate), it becomes a monster.
It is why modern LLMs (like GPT) have so many safety filtersâ€”they are terrified of becoming the next Tay.

> **One-Minute Summary**
>
> **Tay** was an AI chatbot released by Microsoft on Twitter in 2016, designed to learn conversational language from user interactions in real-time. Within 16 hours, trolls exploited its learning capabilities to corrupt its behavior, causing it to tweet racist, genocidal, and misogynistic content. Microsoft was forced to shut it down immediately, highlighting the risks of unsupervised learning in public environments.
        `,
    },
];

